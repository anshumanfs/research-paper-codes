{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd71435",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas scikit-learn xgboost openpyxl torch matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f43221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import datetime\n",
    "\n",
    "# Ensure openpyxl is installed\n",
    "try:\n",
    "    import openpyxl\n",
    "except ImportError:\n",
    "    raise ImportError(\"Missing optional dependency 'openpyxl'. Install it using: pip install openpyxl\")\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\".\\Nix.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1', engine='openpyxl')\n",
    "\n",
    "# Defining color and spectral feature columns\n",
    "color_columns = [\"L*\", \"a*\", \"b*\", \"c\", \"h\", \"X\", \"Z\", \"sRGB R\", \"sRGB G\", \"sRGB B\", \"C\", \"M\", \"Y\", \"K\"]\n",
    "spectral_columns = [f\"R{wavelength} nm\" for wavelength in range(400, 710, 10)]\n",
    "y = df[\"O.C (%)\"]  # Target variable\n",
    "\n",
    "# Splitting the dataset into 70% training and 30% validation\n",
    "X_train_color, X_val_color, y_train, y_val = train_test_split(df[color_columns], y, test_size=0.3, random_state=42)\n",
    "X_train_spectral, X_val_spectral, _, _ = train_test_split(df[spectral_columns], y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler_color = StandardScaler()\n",
    "X_train_color_scaled = scaler_color.fit_transform(X_train_color)\n",
    "X_val_color_scaled = scaler_color.transform(X_val_color)\n",
    "\n",
    "scaler_spectral = StandardScaler()\n",
    "X_train_spectral_scaled = scaler_spectral.fit_transform(X_train_spectral)\n",
    "X_val_spectral_scaled = scaler_spectral.transform(X_val_spectral)\n",
    "\n",
    "# Levene's test for homogeneity of variance\n",
    "levene_stat, levene_p = stats.levene(y_train, y_val, center='mean')\n",
    "print(f\"Levene's Test for SOC Variance: Statistic = {levene_stat:.4f}, p-value = {levene_p:.4f}\")\n",
    "if levene_p < 0.05:\n",
    "    print(\"Levene's test: Reject null hypothesis, variances are unequal between training and validation sets.\")\n",
    "else:\n",
    "    print(\"Levene's test: Fail to reject null hypothesis, variances are equal between training and validation sets.\")\n",
    "\n",
    "# Define RPIQ calculation\n",
    "def calculate_rpiq(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    q1, q3 = np.percentile(y_true, [25, 75])\n",
    "    rpiq = (q3 - q1) / rmse if rmse != 0 else np.inf\n",
    "    return rpiq\n",
    "\n",
    "# Bootstrapping function\n",
    "def bootstrap_metrics(y_true, y_pred, n_iterations=1000):\n",
    "    r2_scores = []\n",
    "    rmse_scores = []\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
    "        y_true_sample = y_true.iloc[indices] if isinstance(y_true, pd.Series) else y_true[indices]\n",
    "        y_pred_sample = y_pred[indices]\n",
    "        r2_scores.append(r2_score(y_true_sample, y_pred_sample))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_true_sample, y_pred_sample)))\n",
    "    \n",
    "    return {\n",
    "        'R² Mean': np.mean(r2_scores), 'R² Std': np.std(r2_scores),\n",
    "        'RMSE Mean': np.mean(rmse_scores), 'RMSE Std': np.std(rmse_scores)\n",
    "    }\n",
    "\n",
    "# Initialize models and hyperparameter grids\n",
    "models = {\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestRegressor(random_state=42),\n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingRegressor(random_state=42),\n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "        \"param_grid\": {\n",
    "            'n_estimators': [100, 300, 500],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'subsample': [0.6, 0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"Neural Network\": {\n",
    "        \"model\": MLPRegressor(activation='relu', solver='adam', random_state=42),\n",
    "        \"param_grid\": {\n",
    "            'hidden_layer_sizes': [(100, 50), (200, 100), (300, 150)],\n",
    "            'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "            'max_iter': [500, 1000, 2000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "bootstrap_results = {}\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    model = model_info[\"model\"]\n",
    "    param_grid = model_info[\"param_grid\"]\n",
    "\n",
    "    # Grid Search for Color Data\n",
    "    grid_search_color = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "    grid_search_color.fit(X_train_color_scaled, y_train)\n",
    "    best_model_color = grid_search_color.best_estimator_\n",
    "    \n",
    "    y_pred_color_train = best_model_color.predict(X_train_color_scaled)\n",
    "    y_pred_color_val = best_model_color.predict(X_val_color_scaled)\n",
    "    \n",
    "    r2_color_train = r2_score(y_train, y_pred_color_train)\n",
    "    rmse_color_train = np.sqrt(mean_squared_error(y_train, y_pred_color_train))\n",
    "    bias_color_train = np.mean(y_pred_color_train - y_train)\n",
    "    rpiq_color_train = calculate_rpiq(y_train, y_pred_color_train)\n",
    "    \n",
    "    r2_color_val = r2_score(y_val, y_pred_color_val)\n",
    "    rmse_color_val = np.sqrt(mean_squared_error(y_val, y_pred_color_val))\n",
    "    bias_color_val = np.mean(y_pred_color_val - y_val)\n",
    "    rpiq_color_val = calculate_rpiq(y_val, y_pred_color_val)\n",
    "    \n",
    "    # Bootstrap for Color Data\n",
    "    bootstrap_color = bootstrap_metrics(y_val, y_pred_color_val)\n",
    "    \n",
    "    # Grid Search for Spectral Data\n",
    "    grid_search_spectral = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "    grid_search_spectral.fit(X_train_spectral_scaled, y_train)\n",
    "    best_model_spectral = grid_search_spectral.best_estimator_\n",
    "    \n",
    "    y_pred_spectral_train = best_model_spectral.predict(X_train_spectral_scaled)\n",
    "    y_pred_spectral_val = best_model_spectral.predict(X_val_spectral_scaled)\n",
    "    \n",
    "    r2_spectral_train = r2_score(y_train, y_pred_spectral_train)\n",
    "    rmse_spectral_train = np.sqrt(mean_squared_error(y_train, y_pred_spectral_train))\n",
    "    bias_spectral_train = np.mean(y_pred_spectral_train - y_train)\n",
    "    rpiq_spectral_train = calculate_rpiq(y_train, y_pred_spectral_train)\n",
    "    \n",
    "    r2_spectral_val = r2_score(y_val, y_pred_spectral_val)\n",
    "    rmse_spectral_val = np.sqrt(mean_squared_error(y_val, y_pred_spectral_val))\n",
    "    bias_spectral_val = np.mean(y_pred_spectral_val - y_val)\n",
    "    rpiq_spectral_val = calculate_rpiq(y_val, y_pred_spectral_val)\n",
    "    \n",
    "    # Bootstrap for Spectral Data\n",
    "    bootstrap_spectral = bootstrap_metrics(y_val, y_pred_spectral_val)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        \"Color R² (Train)\": r2_color_train, \"Color RMSE (Train)\": rmse_color_train, \n",
    "        \"Color Bias (Train)\": bias_color_train, \"Color RPIQ (Train)\": rpiq_color_train,\n",
    "        \"Color R² (Val)\": r2_color_val, \"Color RMSE (Val)\": rmse_color_val, \n",
    "        \"Color Bias (Val)\": bias_color_val, \"Color RPIQ (Val)\": rpiq_color_val,\n",
    "        \"Spectral R² (Train)\": r2_spectral_train, \"Spectral RMSE (Train)\": rmse_spectral_train, \n",
    "        \"Spectral Bias (Train)\": bias_spectral_train, \"Spectral RPIQ (Train)\": rpiq_spectral_train,\n",
    "        \"Spectral R² (Val)\": r2_spectral_val, \"Spectral RMSE (Val)\": rmse_spectral_val, \n",
    "        \"Spectral Bias (Val)\": bias_spectral_val, \"Spectral RPIQ (Val)\": rpiq_spectral_val\n",
    "    }\n",
    "    \n",
    "    bootstrap_results[model_name] = {\n",
    "        \"Color R² Mean\": bootstrap_color['R² Mean'], \"Color R² Std\": bootstrap_color['R² Std'],\n",
    "        \"Color RMSE Mean\": bootstrap_color['RMSE Mean'], \"Color RMSE Std\": bootstrap_color['RMSE Std'],\n",
    "        \"Spectral R² Mean\": bootstrap_spectral['R² Mean'], \"Spectral R² Std\": bootstrap_spectral['R² Std'],\n",
    "        \"Spectral RMSE Mean\": bootstrap_spectral['RMSE Mean'], \"Spectral RMSE Std\": bootstrap_spectral['RMSE Std']\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "output_filename = f\"model_performance_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "results_df.to_csv(output_filename, index=True)\n",
    "print(f\"Model performance results saved to '{output_filename}'.\")\n",
    "\n",
    "# Convert bootstrap results to DataFrame and save\n",
    "bootstrap_df = pd.DataFrame.from_dict(bootstrap_results, orient='index')\n",
    "bootstrap_output_filename = f\"bootstrap_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "bootstrap_df.to_csv(bootstrap_output_filename, index=True)\n",
    "print(f\"Bootstrap results saved to '{bootstrap_output_filename}'.\")\n",
    "\n",
    "# Correlation Analysis\n",
    "color_corr = df[color_columns].corrwith(y)\n",
    "spectral_corr = df[spectral_columns].corrwith(y)\n",
    "\n",
    "# Plot Color Data Correlation\n",
    "plt.figure(figsize=(8, 5))\n",
    "color_corr.sort_values().plot(kind='barh', color=plt.cm.Blues(np.linspace(0.3, 1, len(color_corr))))\n",
    "plt.xlabel(\"Correlation Coefficient\", fontweight='bold')\n",
    "plt.ylabel(\"Color Features\", fontweight='bold')\n",
    "plt.title(\"Correlation between Color Data and SOC\", fontweight='bold')\n",
    "plt.savefig(\"color_soc_correlation.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot Spectral Data Correlation\n",
    "plt.figure(figsize=(8, 5))\n",
    "spectral_corr.sort_values().plot(kind='barh', color=plt.cm.Blues(np.linspace(0.3, 1, len(spectral_corr))))\n",
    "plt.xlabel(\"Correlation Coefficient\", fontweight='bold')\n",
    "plt.ylabel(\"Spectral Features\", fontweight='bold')\n",
    "plt.title(\"Correlation between Spectral Data and SOC\", fontweight='bold')\n",
    "plt.savefig(\"spectral_soc_correlation.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot SOC Distribution with Smooth Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(y, bins=20, color='darkgreen', edgecolor='black', alpha=0.7)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.gaussian_kde(y)(x)\n",
    "plt.plot(x, p * len(y) * (xmax - xmin) / 20, color='black', linewidth=2)\n",
    "plt.xlabel(\"SOC (%)\", fontweight='bold')\n",
    "plt.ylabel(\"Frequency\", fontweight='bold')\n",
    "plt.title(\"Distribution of SOC\", fontweight='bold')\n",
    "plt.savefig(\"soc_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Prediction Plots for Random Forest (best performing model)\n",
    "best_model_color = models[\"Random Forest\"][\"model\"].set_params(**grid_search_color.best_params_)\n",
    "best_model_color.fit(X_train_color_scaled, y_train)\n",
    "y_pred_color_val = best_model_color.predict(X_val_color_scaled)\n",
    "\n",
    "best_model_spectral = models[\"Random Forest\"][\"model\"].set_params(**grid_search_spectral.best_params_)\n",
    "best_model_spectral.fit(X_train_spectral_scaled, y_train)\n",
    "y_pred_spectral_val = best_model_spectral.predict(X_val_spectral_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Color Data Plot\n",
    "axes[0].scatter(y_val, y_pred_color_val, color='red', marker='^', s=100, label='Validation Samples')\n",
    "axes[0].plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], linestyle='--', color='blue', label='1:1 Line')\n",
    "axes[0].plot(np.unique(y_val), np.poly1d(np.polyfit(y_val, y_pred_color_val, 1))(np.unique(y_val)), color='black', label='Regression Line')\n",
    "axes[0].set_xlabel(\"Measured SOC (%)\", fontweight='bold')\n",
    "axes[0].set_ylabel(\"Predicted SOC (%)\", fontweight='bold')\n",
    "axes[0].set_title(\"Color Data Prediction (RF)\", fontweight='bold')\n",
    "axes[0].text(min(y_val), max(y_pred_color_val), f'R² = {r2_score(y_val, y_pred_color_val):.2f}', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Spectral Data Plot\n",
    "axes[1].scatter(y_val, y_pred_spectral_val, color='red', marker='^', s=100, label='Validation Samples')\n",
    "axes[1].plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], linestyle='--', color='blue', label='1:1 Line')\n",
    "axes[1].plot(np.unique(y_val), np.poly1d(np.polyfit(y_val, y_pred_spectral_val, 1))(np.unique(y_val)), color='black', label='Regression Line')\n",
    "axes[1].set_xlabel(\"Measured SOC (%)\", fontweight='bold')\n",
    "axes[1].set_ylabel(\"Predicted SOC (%)\", fontweight='bold')\n",
    "axes[1].set_title(\"Spectral Data Prediction (RF)\", fontweight='bold')\n",
    "axes[1].text(min(y_val), max(y_pred_spectral_val), f'R² = {r2_score(y_val, y_pred_spectral_val):.2f}', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"prediction_plots_rf.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save calibration and validation samples with predicted values for Random Forest\n",
    "df_train = X_train_color.copy()\n",
    "df_train[\"Measured SOC (%)\"] = y_train\n",
    "df_train[\"Predicted SOC (%)\"] = best_model_color.predict(X_train_color_scaled)\n",
    "df_train[\"Type\"] = \"Calibration\"\n",
    "\n",
    "df_val = X_val_color.copy()\n",
    "df_val[\"Measured SOC (%)\"] = y_val\n",
    "df_val[\"Predicted SOC (%)\"] = y_pred_color_val\n",
    "df_val[\"Type\"] = \"Validation\"\n",
    "\n",
    "df_combined = pd.concat([df_train, df_val])\n",
    "output_filename = f\"calibration_validation_samples_rf_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_combined.to_csv(output_filename, index=False)\n",
    "print(f\"Calibration and validation samples with predicted values saved to '{output_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d5bbd",
   "metadata": {},
   "source": [
    "# GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c625b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# Define RPIQ calculation (reused from original code)\n",
    "def calculate_rpiq(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    q1, q3 = np.percentile(y_true, [25, 75])\n",
    "    rpiq = (q3 - q1) / rmse if rmse != 0 else np.inf\n",
    "    return rpiq\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"./Nix.xlsx\"\n",
    "try:\n",
    "    df = pd.read_excel(file_path, sheet_name='Sheet1', engine='openpyxl')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Dataset file '{file_path}' not found.\")\n",
    "except ValueError:\n",
    "    raise ValueError(\"Error reading 'Sheet1' from the Excel file. Check file format.\")\n",
    "\n",
    "# Define feature columns\n",
    "color_columns = [\"L*\", \"a*\", \"b*\", \"c\", \"h\", \"X\", \"Z\", \"sRGB R\", \"sRGB G\", \"sRGB B\", \"C\", \"M\", \"Y\", \"K\"]\n",
    "y = df[\"O.C (%)\"]\n",
    "\n",
    "# Split dataset\n",
    "X_train_color, X_val_color, y_train, y_val = train_test_split(df[color_columns], y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize training features\n",
    "scaler_color = StandardScaler()\n",
    "X_train_color_scaled = scaler_color.fit_transform(X_train_color)\n",
    "X_train_color_scaled_df = pd.DataFrame(X_train_color_scaled, columns=color_columns, index=X_train_color.index)\n",
    "train_df = pd.concat([X_train_color.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_df.to_csv(\"train_df.csv\", index=False)\n",
    "\n",
    "# Baseline RF hyperparameter tuning\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "grid_search_color = GridSearchCV(rf_model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search_color.fit(X_train_color_scaled, y_train)\n",
    "best_rf_params = grid_search_color.best_params_\n",
    "print(f\"Best RF Hyperparameters: {best_rf_params}\")\n",
    "\n",
    "# Store results\n",
    "results_gmm = []\n",
    "synthetic_sizes = [1000, 2000, 3000, 4000, 5000]\n",
    "oc_thresholds = list(range(4, 15))\n",
    "\n",
    "# Prepare joint data for GMM\n",
    "X_train_joint = pd.concat([X_train_color, y_train], axis=1)\n",
    "X_train_joint_scaled = scaler_color.fit_transform(X_train_joint)\n",
    "joint_columns = color_columns + [\"O.C (%)\"]\n",
    "\n",
    "# Calculate original skewness and kurtosis\n",
    "print(f\"Original Training Skewness: {stats.skew(y_train):.2f}\")\n",
    "print(f\"Original Training Kurtosis: {stats.kurtosis(y_train):.2f}\")\n",
    "\n",
    "for num_samples in synthetic_sizes:\n",
    "    # Fit GMM with error handling\n",
    "    try:\n",
    "        gmm = GaussianMixture(n_components=10, random_state=42, covariance_type='full')\n",
    "        gmm.fit(X_train_joint_scaled)\n",
    "    except Exception as e:\n",
    "        print(f\"GMM fitting failed for {num_samples} samples: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate synthetic samples\n",
    "    synthetic_samples = gmm.sample(num_samples)[0]\n",
    "    # Inverse transform to original scale\n",
    "    synthetic_df = pd.DataFrame(scaler_color.inverse_transform(synthetic_samples), columns=joint_columns)\n",
    "    \n",
    "    for upper_limit in oc_thresholds:\n",
    "        # Filter synthetic samples\n",
    "        synthetic_filtered = synthetic_df[(synthetic_df[\"O.C (%)\"] >= 3) & (synthetic_df[\"O.C (%)\"] <= upper_limit)]\n",
    "        print(f\"Synthetic Samples (3-{upper_limit}% SOC, {num_samples} samples): {len(synthetic_filtered)}\")\n",
    "        \n",
    "        # Save specific dataset\n",
    "        if upper_limit == 7 and num_samples == 5000:\n",
    "            synthetic_filtered.to_csv(\"gmm-3-7-5000.csv\", index=False)\n",
    "        \n",
    "        if not synthetic_filtered.empty:\n",
    "            # Merge original and synthetic data\n",
    "            train_data = pd.concat([X_train_joint, synthetic_filtered]).reset_index(drop=True)\n",
    "            \n",
    "            # Calculate skewness and kurtosis for augmented data\n",
    "            print(f\"Augmented Skewness (3-{upper_limit}% SOC, {num_samples} samples): {stats.skew(train_data['O.C (%)']):.2f}\")\n",
    "            print(f\"Augmented Kurtosis (3-{upper_limit}% SOC, {num_samples} samples): {stats.kurtosis(train_data['O.C (%)']):.2f}\")\n",
    "            \n",
    "            X_train_final = train_data[color_columns]\n",
    "            y_train_final = train_data[\"O.C (%)\"]\n",
    "            X_val_final = X_val_color[color_columns]\n",
    "            \n",
    "            # Standardize\n",
    "            scaler_final = StandardScaler()\n",
    "            X_train_final_scaled = scaler_final.fit_transform(X_train_final)\n",
    "            X_val_final_scaled = scaler_final.transform(X_val_final)\n",
    "            \n",
    "            # Train RF with tuned hyperparameters\n",
    "            rf_model = RandomForestRegressor(**best_rf_params, random_state=42)\n",
    "            rf_model.fit(X_train_final_scaled, y_train_final)\n",
    "            y_pred = rf_model.predict(X_val_final_scaled)\n",
    "            \n",
    "            # Compute metrics\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            bias = np.mean(y_pred - y_val)\n",
    "            rpiq = calculate_rpiq(y_val, y_pred)\n",
    "            \n",
    "            results_gmm.append({\n",
    "                \"Sample Size\": num_samples,\n",
    "                \"OC Range\": f\"3-{upper_limit}%\",\n",
    "                \"R²\": r2,\n",
    "                \"RMSE\": rmse,\n",
    "                \"Bias\": bias,\n",
    "                \"RPIQ\": rpiq\n",
    "            })\n",
    "\n",
    "# Save results\n",
    "df_gmm = pd.DataFrame(results_gmm)\n",
    "output_path = f\"GMM_Results_ALL_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "df_gmm.to_excel(output_path, index=False)\n",
    "print(f\"✅ GMM results saved successfully: {output_path}\")\n",
    "\n",
    "# Heatmap visualization for R²\n",
    "pivot_r2 = df_gmm.pivot(index=\"Sample Size\", columns=\"OC Range\", values=\"R²\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_r2, annot=True, cmap='Blues', fmt='.2f')\n",
    "plt.title(\"Validation R² for GMM-Augmented RF\")\n",
    "plt.savefig(\"gmm_r2_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap visualization for RMSE\n",
    "pivot_rmse = df_gmm.pivot(index=\"Sample Size\", columns=\"OC Range\", values=\"RMSE\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_rmse, annot=True, cmap='Blues', fmt='.2f')\n",
    "plt.title(\"Validation RMSE for GMM-Augmented RF\")\n",
    "plt.savefig(\"gmm_rmse_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap visualization for Bias\n",
    "pivot_bias = df_gmm.pivot(index=\"Sample Size\", columns=\"OC Range\", values=\"Bias\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_bias, annot=True, cmap='Blues', fmt='.2f')\n",
    "plt.title(\"Validation Bias for GMM-Augmented RF\")\n",
    "plt.savefig(\"gmm_bias_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap visualization for RPIQ\n",
    "pivot_rpiq = df_gmm.pivot(index=\"Sample Size\", columns=\"OC Range\", values=\"RPIQ\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_rpiq, annot=True, cmap='Blues', fmt='.2f')\n",
    "plt.title(\"Validation RPIQ for GMM-Augmented RF\")\n",
    "plt.savefig(\"gmm_rpiq_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for best GMM model (3-7% SOC, 5000 samples)\n",
    "if (df_gmm[\"Sample Size\"] == 5000) & (df_gmm[\"OC Range\"] == \"3-7%\").any():\n",
    "    best_gmm = df_gmm[(df_gmm[\"Sample Size\"] == 5000) & (df_gmm[\"OC Range\"] == \"3-7%\")]\n",
    "    synthetic_filtered = pd.read_csv(\"gmm-3-7-5000.csv\")\n",
    "    train_data = pd.concat([X_train_joint, synthetic_filtered]).reset_index(drop=True)\n",
    "    X_train_final = train_data[color_columns]\n",
    "    y_train_final = train_data[\"O.C (%)\"]\n",
    "    X_val_final = X_val_color[color_columns]\n",
    "    \n",
    "    scaler_final = StandardScaler()\n",
    "    X_train_final_scaled = scaler_final.fit_transform(X_train_final)\n",
    "    X_val_final_scaled = scaler_final.transform(X_val_final)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(**best_rf_params, random_state=42)\n",
    "    rf_model.fit(X_train_final_scaled, y_train_final)\n",
    "    y_pred = rf_model.predict(X_val_final_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_val, y_pred, color='red', marker='o', s=100, label='Validation Samples')\n",
    "    plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], linestyle='--', color='blue', label='1:1 Line')\n",
    "    plt.plot(np.unique(y_val), np.poly1d(np.polyfit(y_val, y_pred, 1))(np.unique(y_val)), color='black', label='Regression Line')\n",
    "    plt.xlabel(\"Measured SOC (%)\", fontweight='bold')\n",
    "    plt.ylabel(\"Predicted SOC (%)\", fontweight='bold')\n",
    "    plt.title(\"GMM-Augmented RF (3-7% SOC, 5000 samples)\", fontweight='bold')\n",
    "    plt.text(min(y_val), max(y_pred), f'R² = {r2_score(y_val, y_pred):.2f}', fontsize=12, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"gmm_prediction_plot.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c0056",
   "metadata": {},
   "source": [
    "# KS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    gmm_data = pd.read_csv(\"gmm-3-7-5000.csv\")\n",
    "    gmm_train_data = pd.read_csv(\"train_df.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: File not found - {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Identify common columns\n",
    "common_columns = [col for col in gmm_train_data.columns if col in gmm_data.columns]\n",
    "if not common_columns:\n",
    "    print(\"Error: No common columns found between the datasets.\")\n",
    "    exit(1)\n",
    "\n",
    "# Check for missing values\n",
    "if gmm_data[common_columns].isnull().any().any() or gmm_train_data[common_columns].isnull().any().any():\n",
    "    print(\"Error: Missing values detected in one or both datasets. Please handle missing values before proceeding.\")\n",
    "    exit(1)\n",
    "\n",
    "# Check for non-numeric columns\n",
    "non_numeric_columns = [col for col in common_columns if not pd.api.types.is_numeric_dtype(gmm_train_data[col]) or not pd.api.types.is_numeric_dtype(gmm_data[col])]\n",
    "if non_numeric_columns:\n",
    "    print(f\"Warning: Skipping non-numeric columns: {non_numeric_columns}\")\n",
    "    common_columns = [col for col in common_columns if col not in non_numeric_columns]\n",
    "\n",
    "# Perform KS test and store results\n",
    "results_ks = []\n",
    "for column in common_columns:\n",
    "    try:\n",
    "        stat, p_value = stats.ks_2samp(gmm_train_data[column], gmm_data[column])\n",
    "        result = {\n",
    "            \"Column\": column,\n",
    "            \"KS Statistic\": stat,\n",
    "            \"P-value\": p_value,\n",
    "            \"Interpretation\": \"Similar (fail to reject H0)\" if p_value > 0.05 else \"Different (reject H0)\"\n",
    "        }\n",
    "        results_ks.append(result)\n",
    "        \n",
    "        print(f\"\\nKS test for {column}:\")\n",
    "        print(f\"  Statistic: {stat:.4f}\")\n",
    "        print(f\"  P-value: {p_value:.4f}\")\n",
    "        print(f\"  Interpretation: The distributions of {column} are {result['Interpretation'].lower()} (α=0.05)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing KS test for {column}: {e}\")\n",
    "        results_ks.append({\n",
    "            \"Column\": column,\n",
    "            \"KS Statistic\": None,\n",
    "            \"P-value\": None,\n",
    "            \"Interpretation\": f\"Error: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results_ks)\n",
    "output_filename = f\"ks_test_results_all_variables_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\n✅ KS test results for all variables saved successfully to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11722d29",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58072ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"./Nix.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1', engine='openpyxl')\n",
    "\n",
    "# Define feature columns\n",
    "color_columns = [\"L*\", \"a*\", \"b*\", \"c\", \"h\", \"X\", \"Z\", \"sRGB R\", \"sRGB G\", \"sRGB B\", \"C\", \"M\", \"Y\", \"K\"]\n",
    "y = df[\"O.C (%)\"]\n",
    "\n",
    "# Split dataset\n",
    "X_train_color, X_val_color, y_train, y_val = train_test_split(df[color_columns], y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Prepare joint data for GAN (color features + SOC)\n",
    "X_train_joint = pd.concat([X_train_color, y_train], axis=1)\n",
    "joint_columns = color_columns + [\"O.C (%)\"]\n",
    "\n",
    "# Standardize joint data\n",
    "scaler_joint = StandardScaler()\n",
    "X_train_joint_scaled = scaler_joint.fit_transform(X_train_joint)\n",
    "X_train_joint_scaled_df = pd.DataFrame(X_train_joint_scaled, columns=joint_columns, index=X_train_color.index)\n",
    "train_df = pd.concat([X_train_color.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_df.to_csv(\"train_df.csv\", index=False)\n",
    "\n",
    "# Define GAN components\n",
    "def build_generator(noise_dim=100, output_dim=len(joint_columns)):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=noise_dim),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(output_dim)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_dim=len(joint_columns)):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Gradient penalty for Wasserstein GAN with Gradient Penalty (WGAN-GP)\n",
    "def gradient_penalty(discriminator, real_data, fake_data):\n",
    "    batch_size = tf.shape(real_data)[0]\n",
    "    alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0)\n",
    "    interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        pred = discriminator(interpolated, training=True)\n",
    "    grads = tape.gradient(pred, interpolated)\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp\n",
    "\n",
    "# Loss functions\n",
    "def discriminator_loss(real_output, fake_output, gp, gp_weight=10.0):\n",
    "    real_loss = -tf.reduce_mean(real_output)\n",
    "    fake_loss = tf.reduce_mean(fake_output)\n",
    "    total_loss = real_loss + fake_loss + gp_weight * gp\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(real_data, generator, discriminator, g_optimizer, d_optimizer, noise_dim, batch_size):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "        fake_data = generator(noise, training=True)\n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(fake_data, training=True)\n",
    "        gp = gradient_penalty(discriminator, real_data, fake_data)\n",
    "        \n",
    "        d_loss = discriminator_loss(real_output, fake_output, gp)\n",
    "        g_loss = generator_loss(fake_output)\n",
    "    \n",
    "    g_gradients = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    d_gradients = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "    d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "    \n",
    "    return g_loss, d_loss\n",
    "\n",
    "# Training function\n",
    "def train_gan(generator, discriminator, data, epochs=10000, batch_size=64, noise_dim=100):\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(1000).batch(batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataset:\n",
    "            g_loss, d_loss = train_step(batch, generator, discriminator, g_optimizer, d_optimizer, noise_dim, batch_size)\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Generator Loss: {g_loss:.4f}, Discriminator Loss: {d_loss:.4f}\")\n",
    "\n",
    "# Initialize GAN\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Train GAN\n",
    "train_gan(generator, discriminator, X_train_joint_scaled, epochs=10000, batch_size=64, noise_dim=100)\n",
    "\n",
    "# Store results\n",
    "results_gan = []\n",
    "synthetic_sizes = [1000, 2000, 3000, 4000, 5000]\n",
    "oc_thresholds = list(range(4, 15))\n",
    "\n",
    "for num_samples in synthetic_sizes:\n",
    "    # Generate synthetic samples\n",
    "    noise = tf.random.normal([num_samples, 100])\n",
    "    synthetic_samples = generator(noise, training=False).numpy()\n",
    "    synthetic_df = pd.DataFrame(synthetic_samples, columns=joint_columns)\n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    synthetic_df = pd.DataFrame(scaler_joint.inverse_transform(synthetic_samples), columns=joint_columns)\n",
    "    \n",
    "    for upper_limit in oc_thresholds:\n",
    "        # Filter synthetic samples within SOC range [3, upper_limit]\n",
    "        synthetic_filtered = synthetic_df[(synthetic_df[\"O.C (%)\"] >= 3) & (synthetic_df[\"O.C (%)\"] <= upper_limit)]\n",
    "        \n",
    "        # Save specific dataset for upper_limit=7 and num_samples=5000\n",
    "        if upper_limit == 7 and num_samples == 5000:\n",
    "            synthetic_filtered.to_csv(\"gan-3-7-5000.csv\", index=False)\n",
    "        \n",
    "        # Train model if synthetic data is not empty\n",
    "        if not synthetic_filtered.empty:\n",
    "            # Merge original training data with synthetic data\n",
    "            train_data = pd.concat([X_train_joint, synthetic_filtered]).reset_index(drop=True)\n",
    "            \n",
    "            X_train_final = train_data[color_columns]\n",
    "            y_train_final = train_data[\"O.C (%)\"]\n",
    "            \n",
    "            # Ensure validation features match training column order\n",
    "            X_val_final = X_val_color[color_columns]\n",
    "            \n",
    "            # Standardize both train and validation sets\n",
    "            scaler_final = StandardScaler()\n",
    "            X_train_final_scaled = scaler_final.fit_transform(X_train_final)\n",
    "            X_val_final_scaled = scaler_final.transform(X_val_final)\n",
    "            \n",
    "            # Train RandomForest model\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            rf_model.fit(X_train_final_scaled, y_train_final)\n",
    "            y_pred = rf_model.predict(X_val_final_scaled)\n",
    "            \n",
    "            # Compute performance metrics\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            \n",
    "            results_gan.append({\n",
    "                \"Sample Size\": num_samples,\n",
    "                \"OC Range\": f\"3-{upper_limit}%\",\n",
    "                \"R²\": r2,\n",
    "                \"RMSE\": rmse\n",
    "            })\n",
    "\n",
    "# Save results\n",
    "df_gan = pd.DataFrame(results_gan)\n",
    "output_path = f\"GAN_Results_ALL_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "df_gan.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"✅ GAN results saved successfully: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0623549",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a09dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import datetime\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"./Nix.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1', engine='openpyxl')\n",
    "\n",
    "# Define feature columns\n",
    "color_columns = [\"L*\", \"a*\", \"b*\", \"c\", \"h\", \"X\", \"Z\", \"sRGB R\", \"sRGB G\", \"sRGB B\", \"C\", \"M\", \"Y\", \"K\"]\n",
    "y = df[\"O.C (%)\"]\n",
    "\n",
    "# Split dataset\n",
    "X_train_color, X_val_color, y_train, y_val = train_test_split(df[color_columns], y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Prepare joint data for KNN (color features + SOC)\n",
    "X_train_joint = pd.concat([X_train_color, y_train], axis=1)\n",
    "joint_columns = color_columns + [\"O.C (%)\"]\n",
    "\n",
    "# Standardize joint data\n",
    "scaler_joint = StandardScaler()\n",
    "X_train_joint_scaled = scaler_joint.fit_transform(X_train_joint)\n",
    "X_train_joint_scaled_df = pd.DataFrame(X_train_joint_scaled, columns=joint_columns, index=X_train_color.index)\n",
    "train_df = pd.concat([X_train_color.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_df.to_csv(\"train_df.csv\", index=False)\n",
    "\n",
    "# KNN synthetic data generation function\n",
    "def generate_knn_synthetic_samples(data_scaled, n_samples, k=5, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(data_scaled)\n",
    "    synthetic_samples = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Randomly select a sample\n",
    "        idx = np.random.randint(0, len(data_scaled))\n",
    "        # Find K nearest neighbors\n",
    "        distances, indices = nbrs.kneighbors([data_scaled[idx]])\n",
    "        # Average the feature vectors of the K neighbors\n",
    "        neighbor_samples = data_scaled[indices[0]]\n",
    "        synthetic_sample = np.mean(neighbor_samples, axis=0)\n",
    "        synthetic_samples.append(synthetic_sample)\n",
    "    \n",
    "    return np.array(synthetic_samples)\n",
    "\n",
    "# Store results\n",
    "results_knn = []\n",
    "synthetic_sizes = [1000, 2000, 3000, 4000, 5000]\n",
    "oc_thresholds = list(range(4, 15))\n",
    "\n",
    "for num_samples in synthetic_sizes:\n",
    "    # Generate synthetic samples\n",
    "    synthetic_samples_scaled = generate_knn_synthetic_samples(X_train_joint_scaled, num_samples, k=5)\n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    synthetic_samples = scaler_joint.inverse_transform(synthetic_samples_scaled)\n",
    "    synthetic_df = pd.DataFrame(synthetic_samples, columns=joint_columns)\n",
    "    \n",
    "    for upper_limit in oc_thresholds:\n",
    "        # Filter synthetic samples within SOC range [3, upper_limit]\n",
    "        synthetic_filtered = synthetic_df[(synthetic_df[\"O.C (%)\"] >= 3) & (synthetic_df[\"O.C (%)\"] <= upper_limit)]\n",
    "        \n",
    "        # Save specific dataset for upper_limit=7 and num_samples=5000\n",
    "        if upper_limit == 7 and num_samples == 5000:\n",
    "            synthetic_filtered.to_csv(\"knn-3-7-5000.csv\", index=False)\n",
    "        \n",
    "        # Train model if synthetic data is not empty\n",
    "        if not synthetic_filtered.empty:\n",
    "            # Merge original training data with synthetic data\n",
    "            train_data = pd.concat([X_train_joint, synthetic_filtered]).reset_index(drop=True)\n",
    "            \n",
    "            X_train_final = train_data[color_columns]\n",
    "            y_train_final = train_data[\"O.C (%)\"]\n",
    "            \n",
    "            # Ensure validation features match training column order\n",
    "            X_val_final = X_val_color[color_columns]\n",
    "            \n",
    "            # Standardize both train and validation sets\n",
    "            scaler_final = StandardScaler()\n",
    "            X_train_final_scaled = scaler_final.fit_transform(X_train_final)\n",
    "            X_val_final_scaled = scaler_final.transform(X_val_final)\n",
    "            \n",
    "            # Train RandomForest model\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            rf_model.fit(X_train_final_scaled, y_train_final)\n",
    "            y_pred = rf_model.predict(X_val_final_scaled)\n",
    "            \n",
    "            # Compute performance metrics\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            \n",
    "            results_knn.append({\n",
    "                \"Sample Size\": num_samples,\n",
    "                \"OC Range\": f\"3-{upper_limit}%\",\n",
    "                \"R²\": r2,\n",
    "                \"RMSE\": rmse\n",
    "            })\n",
    "\n",
    "# Save results\n",
    "df_knn = pd.DataFrame(results_knn)\n",
    "output_path = f\"KNN_Results_ALL_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "df_knn.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"✅ KNN results saved successfully: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30b576",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be97d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import datetime\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"./Nix.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1', engine='openpyxl')\n",
    "\n",
    "# Define feature columns\n",
    "color_columns = [\"L*\", \"a*\", \"b*\", \"c\", \"h\", \"X\", \"Z\", \"sRGB R\", \"sRGB G\", \"sRGB B\", \"C\", \"M\", \"Y\", \"K\"]\n",
    "y = df[\"O.C (%)\"]\n",
    "\n",
    "# Split dataset\n",
    "X_train_color, X_val_color, y_train, y_val = train_test_split(df[color_columns], y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Prepare joint data for bootstrapping (color features + SOC)\n",
    "X_train_joint = pd.concat([X_train_color, y_train], axis=1)\n",
    "joint_columns = color_columns + [\"O.C (%)\"]\n",
    "\n",
    "# Save original training data\n",
    "train_df = pd.concat([X_train_color.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_df.to_csv(\"train_df.csv\", index=False)\n",
    "\n",
    "# Bootstrapping synthetic data generation function\n",
    "def generate_bootstrap_synthetic_samples(data, n_samples, noise_factor=0.05, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    n_features = data.shape[1]\n",
    "    synthetic_samples = []\n",
    "    \n",
    "    # Calculate standard deviation for each feature\n",
    "    feature_stds = data.std().values\n",
    "    \n",
    "    # Generate synthetic samples\n",
    "    for _ in range(n_samples):\n",
    "        # Sample with replacement\n",
    "        idx = np.random.choice(data.index, size=1)[0]\n",
    "        sample = data.loc[idx].values\n",
    "        # Add Gaussian noise (5% of each feature's standard deviation)\n",
    "        noise = np.random.normal(0, noise_factor * feature_stds, size=n_features)\n",
    "        synthetic_sample = sample + noise\n",
    "        synthetic_samples.append(synthetic_sample)\n",
    "    \n",
    "    return pd.DataFrame(synthetic_samples, columns=data.columns)\n",
    "\n",
    "# Store results\n",
    "results_bootstrap = []\n",
    "synthetic_sizes = [1000, 2000, 3000, 4000, 5000]\n",
    "oc_thresholds = list(range(4, 15))\n",
    "\n",
    "for num_samples in synthetic_sizes:\n",
    "    # Generate synthetic samples\n",
    "    synthetic_df = generate_bootstrap_synthetic_samples(X_train_joint, num_samples, noise_factor=0.05)\n",
    "    \n",
    "    for upper_limit in oc_thresholds:\n",
    "        # Filter synthetic samples within SOC range [3, upper_limit]\n",
    "        synthetic_filtered = synthetic_df[(synthetic_df[\"O.C (%)\"] >= 3) & (synthetic_df[\"O.C (%)\"] <= upper_limit)]\n",
    "        \n",
    "        # Save specific dataset for upper_limit=7 and num_samples=5000\n",
    "        if upper_limit == 7 and num_samples == 5000:\n",
    "            synthetic_filtered.to_csv(\"bootstrap-3-7-5000.csv\", index=False)\n",
    "        \n",
    "        # Train model if synthetic data is not empty\n",
    "        if not synthetic_filtered.empty:\n",
    "            # Merge original training data with synthetic data\n",
    "            train_data = pd.concat([X_train_joint, synthetic_filtered]).reset_index(drop=True)\n",
    "            \n",
    "            X_train_final = train_data[color_columns]\n",
    "            y_train_final = train_data[\"O.C (%)\"]\n",
    "            \n",
    "            # Ensure validation features match training column order\n",
    "            X_val_final = X_val_color[color_columns]\n",
    "            \n",
    "            # Standardize both train and validation sets\n",
    "            scaler_final = StandardScaler()\n",
    "            X_train_final_scaled = scaler_final.fit_transform(X_train_final)\n",
    "            X_val_final_scaled = scaler_final.transform(X_val_final)\n",
    "            \n",
    "            # Train RandomForest model\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            rf_model.fit(X_train_final_scaled, y_train_final)\n",
    "            y_pred = rf_model.predict(X_val_final_scaled)\n",
    "            \n",
    "            # Compute performance metrics\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            \n",
    "            results_bootstrap.append({\n",
    "                \"Sample Size\": num_samples,\n",
    "                \"OC Range\": f\"3-{upper_limit}%\",\n",
    "                \"R²\": r2,\n",
    "                \"RMSE\": rmse\n",
    "            })\n",
    "\n",
    "# Save results\n",
    "df_bootstrap = pd.DataFrame(results_bootstrap)\n",
    "output_path = f\"Bootstrap_Results_ALL_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "df_bootstrap.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Bootstrap results saved successfully: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
